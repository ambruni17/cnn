{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION TO CONVOLUTIONAL NEURAL NETWORKS (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional neural network (ConvNets or CNNs) is widely used for  \n",
    "\n",
    "- **images recognition,** \n",
    "- **images classifications**\n",
    "- **Objects detections,**\n",
    "- **recognition faces **etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cnn_steps.png\" width=80%/>\n",
    "\n",
    "source:https://medium.com/@Aj.Cheng/convolutional-neural-network-d9f69e473feb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components to define a basic convolutional network.\n",
    "\n",
    "- **Convolutional layer**\n",
    "- **Pooling layer**\n",
    "- **Fully Connected layer**\n",
    "- **Output layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is the first layer to extract features from an input image. this layer learn the relationship between pixels by element wise matrix multiplication operation between Original image, and the other is the filter or kernel that turns the image into feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mnist with cnn and data aug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,Convolution2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[100],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using Tensorflow, the format should be (batch, height, width, channels). If we are using Theano, the format should be (batch, channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reshapeing our data into format of(s,h,w,c)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "\n",
    "#feature scaling\n",
    "X_train = X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "\n",
    "X_train /=255\n",
    "X_test /=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to one-hot encode the labels i.e. Y_train and Y_test. \n",
    "\n",
    "\n",
    "Y_train[0] = [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.] since the label representated by it is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert our data into one hot encoding form\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/vinit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  #create an object for this seq. \n",
    "\n",
    "model.add(Convolution2D(32,3,3,activation='relu',input_shape=(28,28,1)))  # first conv. layer\n",
    "#no_filter,filter_size,a_f,input_shape\n",
    "\n",
    "model.add(MaxPooling2D(2,2))   #max pooling layer\n",
    "\n",
    "model.add(Convolution2D(64,3,3,activation='relu'))   #second conv. layer\n",
    "\n",
    "model.add(MaxPooling2D(2,2))    #max pooling layer\n",
    "\n",
    "model.add(Dropout(0.25))     # dropout layer\n",
    "\n",
    "\n",
    "model.add(Flatten())      \n",
    "\n",
    "model.add(Dense(1000,activation='relu'))   #first hidden layer\n",
    "\n",
    "model.add(Dropout(0.5))   ## dropout layer\n",
    "\n",
    "model.add(Dense(10,activation='softmax'))  #output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*3*3+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1601000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,629,826\n",
      "Trainable params: 1,629,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using the categorical_crossentropy loss, our targets should be in categorical format (e.g. if we have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for a 1 at the index corresponding to the class of the sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If there were 2 classes, we would have used binary_crossentropy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is responsible for updating the weights of the neurons via backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce over-fitting, we use another technique known as Data Augmentation. Data augmentation rotates, shears, zooms, etc the image so that the model learns to generalize well and not remember specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data Aug.\n",
    "##we generate random images with some manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(shear_range=.2,\n",
    "                               rotation_range=20,\n",
    "                               width_shift_range=0.2,\n",
    "                               height_shift_range=0.2,\n",
    "                               zoom_range=.2,\n",
    "                               horizontal_flip=True)\n",
    "\n",
    "test_gen = ImageDataGenerator() #validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_gen.flow(X_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using batch of 64, so the model will take 64 images at a time and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 or 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "710/937 [=====================>........] - ETA: 14s - loss: 0.9976 - accuracy: 0.6582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d32797dcc54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_steps=10000/64)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,steps_per_epoch=60000/64,\n",
    "                    epochs=15,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=10000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_gen1=ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
    "#                              height_shift_range=0.08, zoom_range=0.08,horizontal_flip=True)\n",
    "\n",
    "#test_gen1 = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_generator1 = train_gen1.flow(X_train, y_train, batch_size=64)\n",
    "#test_generator1 = test_gen1.flow(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit_generator(train_generator1,steps_per_epoch=60000/64,epochs=10,validation_data=test_generator1,validation_steps=10000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_cnn.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('mnist_model.h5')\n",
    "#loaded_model = load_model('mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2    #  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread('2test.png')   #read the image ,0-gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 163, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape  # will be array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f969b392210>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.invert(img)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f969b376810>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img, (28, 28))   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f969b2e90d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(img,'sample.png')  #write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(1,28,28,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict_classes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict_classes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create some complex cnn model\n",
    "## data augmentation:\n",
    "## pretrained model (transfer learning)\n",
    "\n",
    "##fashion mnist data(10,gray images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
